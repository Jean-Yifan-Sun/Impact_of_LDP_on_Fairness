{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from multi_freq_ldpy.pure_frequency_oracles.GRR import GRR_Client\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_seed = 20\n",
    "\n",
    "n_splits = 10 # cross validation \n",
    "\n",
    "# Reading dataset\n",
    "dataset = 'adult_50000' # ['adult_10000','adult_27260', 'adult_50000']\n",
    "\n",
    "# Target attribute\n",
    "target = 'income' \n",
    "\n",
    "# Protected attribute \n",
    "protected_att = 'gender'\n",
    "\n",
    "# Privacy settings\n",
    "dict_setting = {\"indsLDP\": [protected_att,'age','race','maritalstatus','nativecountry'],\n",
    "                \"combsLDP\": [protected_att,'age','race','maritalstatus','nativecountry']}\n",
    "\n",
    "# Folder where results are saved as csv files\n",
    "results_path = 'Results/'\n",
    "# Fariness metrics and info no privacy results\n",
    "header_nldp = [\"Seed_num\",\"SP_maj\", \"SP_min\", \"SPD\",\"EO_maj\",\"EO_min\",\"EOD\",\"PE_maj\", \"PE_min\", \"PED\",\"TE_maj\",\"TE_min\",\"TED\",\"OA_maj\", \"OA_min\", \"OAD\",\"PRP_maj\",\"PRP_min\",\"PRPD\",\"CSP_B0_maj\", \"CSP_B0_min\", \"CSD_B0\",\"CSP_B1_maj\", \"CSP_B1_min\", \"CSD_B1\",\"Acc\"]\n",
    "# Fariness metrics and info for privacy settings\n",
    "header_ldp = [\"Seed_num\",\"epsilon\",\"SP_maj\", \"SP_min\", \"SPD\",\"EO_maj\",\"EO_min\",\"EOD\",\"PE_maj\", \"PE_min\", \"PED\",\"TE_maj\",\"TE_min\",\"TED\",\"OA_maj\", \"OA_min\", \"OAD\",\"PRP_maj\",\"PRP_min\",\"PRPD\",\"CSP_B0_maj\", \"CSP_B0_min\", \"CSD_B0\",\"CSP_B1_maj\", \"CSP_B1_min\", \"CSD_B1\",\"Acc\"]\n",
    "\n",
    "# Epsilon values for privacy\n",
    "lst_eps =  [16,8,5,4,3,2,1,0.5,0.1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needed functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv function for saving the results\n",
    "def write_to_csv(setting,dataset,header):\n",
    "    with open(results_path + dataset + '_'+ setting +'_results.csv', mode='a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)    \n",
    "        writer.writerow(header)   \n",
    "    file.close()\n",
    "    \n",
    "\n",
    "def confusion_matrix_scorer(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred, labels = [0, 1])\n",
    "    \n",
    "    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n",
    "            'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "\n",
    "# Functions for fairness metrics\n",
    "\n",
    "def Statistical_parity(y_pred):\n",
    "    PAR_F =[]\n",
    "    for row in y_pred:\n",
    "        x = np.count_nonzero(row == 1)\n",
    "        result = x/len(row)\n",
    "        PAR_F.append(result)\n",
    "    PAR_F_mean = np.mean(PAR_F, axis=0)\n",
    "    return PAR_F_mean\n",
    "\n",
    "def Metric_disparity(x, y):\n",
    "    return x - y\n",
    "\n",
    "def Equal_opportunity(cm):\n",
    "    TPR_mean = []\n",
    "    for row in cm:\n",
    "        TP = row['tp']\n",
    "        FN = row['fn']\n",
    "        TPR = TP/float(TP+FN)\n",
    "    TPR_mean.append(TPR)\n",
    "    return np.mean(TPR_mean, axis=0)\n",
    "\n",
    "\n",
    "def Predictive_equality(cm):\n",
    "    FPR_mean = []\n",
    "    for row in cm:\n",
    "        FP = row['fp']\n",
    "        TN = row['tn']\n",
    "        FPR = FP/float(FP+TN) \n",
    "    FPR_mean.append(FPR)\n",
    "    return np.mean(FPR_mean, axis=0)\n",
    "\n",
    "def Treatment_equality(cm):\n",
    "    TE_mean = []\n",
    "    for row in cm:\n",
    "        FP = row['fp']\n",
    "        FN = row['fn']\n",
    "        TE = FN/float(FP)\n",
    "        TE_mean.append(TE)\n",
    "    return np.mean(TE_mean, axis=0)\n",
    "\n",
    "def Overall_accuracy(y_test, y_pred):\n",
    "    OA_mean = []\n",
    "    for (row_test,row_pred) in zip(y_test, y_pred):\n",
    "        accuracy = accuracy_score(row_test, row_pred)\n",
    "        OA_mean.append(accuracy)\n",
    "    return np.mean(OA_mean, axis=0)\n",
    "\n",
    "def Predictive_rate_parity(y_test,y_pred):\n",
    "    precision_mean = []\n",
    "    for (row_test,row_pred) in zip(y_test,y_pred):\n",
    "        precision = precision_score(row_test, row_pred)\n",
    "        precision_mean.append(precision)\n",
    "    return np.mean(precision_mean, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_num</th>\n",
       "      <th>maritalstatus</th>\n",
       "      <th>nativecountry</th>\n",
       "      <th>hoursperweek</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45844</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45845</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45846</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45847</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45848</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45849 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass  education_num  maritalstatus  nativecountry  \\\n",
       "0        1          1              1              1              1   \n",
       "1        0          1              1              0              1   \n",
       "2        0          1              0              0              1   \n",
       "3        1          1              1              1              0   \n",
       "4        0          1              1              0              1   \n",
       "...    ...        ...            ...            ...            ...   \n",
       "45844    0          1              1              1              0   \n",
       "45845    1          0              1              1              0   \n",
       "45846    0          1              1              0              1   \n",
       "45847    0          1              1              0              1   \n",
       "45848    1          0              1              1              1   \n",
       "\n",
       "       hoursperweek  gender  race  income  \n",
       "0                 0       0     1       0  \n",
       "1                 0       1     1       0  \n",
       "2                 0       1     1       0  \n",
       "3                 1       1     0       0  \n",
       "4                 0       1     1       0  \n",
       "...             ...     ...   ...     ...  \n",
       "45844             1       1     1       1  \n",
       "45845             1       1     0       0  \n",
       "45846             1       1     1       0  \n",
       "45847             0       0     1       0  \n",
       "45848             0       0     0       0  \n",
       "\n",
       "[45849 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/' + dataset + '.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for no privacy setting (baseline setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That took 235.4098448753357 seconds\n"
     ]
    }
   ],
   "source": [
    "#folder_name of the results\n",
    "setting = 'NoLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_nldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "# Evaluating nb_seed times due to randomness\n",
    "np_acc = []\n",
    "np_sp_min, np_sp_maj = [], []\n",
    "np_tv = []\n",
    "np_eo_min, np_eo_maj = [], []\n",
    "np_eod = []\n",
    "np_pe_min, np_pe_maj = [], []\n",
    "np_ped = []\n",
    "np_te_min, np_te_maj = [], []\n",
    "np_ted = []\n",
    "np_oa_min, np_oa_maj = [], []\n",
    "np_oad = []\n",
    "np_prp_min, np_prp_maj = [], []\n",
    "np_prpd = []\n",
    "np_csp_min_B0, np_csp_maj_B0 = [], []\n",
    "np_csd_B0 = []\n",
    "np_csp_min_B1, np_csp_maj_B1 = [], []\n",
    "np_csd_B1 = []\n",
    "\n",
    "for seed in range(nb_seed):\n",
    "\n",
    "    # Preparing X and y using pandas\n",
    "    X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "    y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "    \n",
    "    kf = StratifiedKFold(n_splits = n_splits,shuffle=True)\n",
    "\n",
    "    pred = []\n",
    "    pred_min = []\n",
    "    pred_maj = []\n",
    "    pred_A0_B0 = []\n",
    "    pred_A0_B1 = []\n",
    "    pred_A1_B0 = []\n",
    "    pred_A1_B1 = []\n",
    "    pred_proba_F = []\n",
    "    pred_proba_M = []\n",
    "    matrix_min = []\n",
    "    matrix_maj = []\n",
    "    test_min = []\n",
    "    test_maj = []\n",
    "    acc = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # instantiate and train the model\n",
    "        model = RandomForestClassifier(n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test) # prediction of the actual samples\n",
    "        pred.append(y_pred)\n",
    "        acc.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # retrieving minority, majority from the test set\n",
    "        X_test_min, X_test_maj = X_test[X_test[protected_att] == 0], X_test[X_test[protected_att] == 1]\n",
    "\n",
    "        # predicted outcomes for minority, majority\n",
    "        y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "        pred_min.append(y_pred_min)\n",
    "        pred_maj.append(y_pred_maj)\n",
    "\n",
    "        # predicted probabilities for minority, majority\n",
    "        y_pred_proba_min, y_pred_proba_maj = model.predict_proba(X_test_min)[:,1], model.predict_proba(X_test_maj)[:,1]\n",
    "        pred_proba_F.append(y_pred_proba_min)\n",
    "        pred_proba_M.append(y_pred_proba_maj)\n",
    "\n",
    "        indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "        y_test_min, y_test_maj = y_test.get(key = indices_min), y_test.get(key = indices_maj)\n",
    "        test_min.append(y_test_min)\n",
    "        test_maj.append(y_test_maj)\n",
    "\n",
    "        # confusion matrix for minority, majority\n",
    "        conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min,y_pred_min), confusion_matrix_scorer(y_test_maj,y_pred_maj)\n",
    "        matrix_min.append(conf_matrix_min)\n",
    "        matrix_maj.append(conf_matrix_maj)\n",
    "\n",
    "        # Needed for the computation of Cond.Stat.Disp\n",
    "        # retrieving four groups: A=0_B=0, A0_B=1, A1_B=0, A1_B=1 from the test set\n",
    "        X_test_min_B0, X_test_min_B1, X_test_maj_B0, X_test_maj_B1 = X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 1)],X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 1)]\n",
    "\n",
    "\n",
    "# predicted outcomes for the four groups\n",
    "    y_pred_min_B0, y_pred_min_B1, y_pred_maj_B0, y_pred_maj_B1 = model.predict(X_test_min_B0), model.predict(X_test_min_B1), model.predict(X_test_maj_B0), model.predict(X_test_maj_B1)  \n",
    "    pred_A0_B0.append(y_pred_min_B0)\n",
    "    pred_A0_B1.append(y_pred_min_B1)\n",
    "    pred_A1_B0.append(y_pred_maj_B0)\n",
    "    pred_A1_B1.append(y_pred_maj_B1)\n",
    "\n",
    "    # fairness metrics\n",
    "    np_sp_min.append(Statistical_parity(pred_min))\n",
    "    np_sp_maj.append(Statistical_parity(pred_maj))\n",
    "    np_tv.append(Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)))\n",
    "    np_eo_min.append(Equal_opportunity(matrix_min))\n",
    "    np_eo_maj.append(Equal_opportunity(matrix_maj))\n",
    "    np_eod.append(Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)))\n",
    "    np_pe_min.append(Predictive_equality(matrix_min))\n",
    "    np_pe_maj.append(Predictive_equality(matrix_maj))\n",
    "    np_ped.append(Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)))\n",
    "    np_te_min.append(Treatment_equality(matrix_min))\n",
    "    np_te_maj.append(Treatment_equality(matrix_maj))\n",
    "    np_ted.append(Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)))\n",
    "    np_oa_min.append(Overall_accuracy(test_min, pred_min))\n",
    "    np_oa_maj.append(Overall_accuracy(test_maj, pred_maj))\n",
    "    np_oad.append(Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)))\n",
    "    np_prp_min.append(Predictive_rate_parity(test_min, pred_min))\n",
    "    np_prp_maj.append(Predictive_rate_parity(test_maj, pred_maj))\n",
    "    np_prpd.append(Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min))) \n",
    "    np_acc.append(np.mean(acc))\n",
    "    np_csp_min_B0 = Statistical_parity(pred_A0_B0)\n",
    "    np_csp_maj_B0 = Statistical_parity(pred_A1_B0)\n",
    "    np_csd_B0.append(Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)))\n",
    "    np_csp_min_B1 = Statistical_parity(pred_A0_B1)\n",
    "    np_csp_maj_B1 = Statistical_parity(pred_A1_B1)\n",
    "    np_csd_B1.append(Metric_disparity(Statistical_parity(pred_A1_B1), Statistical_parity(pred_A0_B1)))\n",
    "    #writing the results to the csv file\n",
    "    write_to_csv(setting,dataset,[str(seed), Statistical_parity(pred_maj),Statistical_parity(pred_min),Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)),Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min), Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)),Predictive_equality(matrix_maj), Predictive_equality(matrix_min),Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)),Treatment_equality(matrix_maj), Treatment_equality(matrix_min), Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)),Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min),Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)),Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min), Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min)),Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0),Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)), Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1),Metric_disparity(Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1)),np.mean(acc)])\n",
    "write_to_csv(setting,dataset,[20,np.std(np_sp_maj),np.std(np_sp_min),np.std(np_tv),np.std(np_eo_maj),np.std(np_eo_min),np.std(np_eod),np.std(np_pe_maj),np.std(np_pe_min),np.std(np_ped),np.std(np_te_maj),np.std(np_te_min),np.std(np_ted),np.std(np_oa_maj),np.std(np_oa_min),np.std(np_oad),np.std(np_prp_maj),np.std(np_prp_min),np.std(np_prpd),np.std(np_csp_maj_B0),np.std(np_csp_min_B0),np.std(np_csd_B0),np.std(np_csp_maj_B1),np.std(np_csp_min_B1),np.std(np_csd_B1),np.std(np_acc)])\n",
    "print('That took {} seconds'.format(time.time() - starttime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDP setting 1: Only the protected attribute is obfuscated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0.5\n",
      "0.1\n",
      "That took 2740.9875667095184 seconds\n"
     ]
    }
   ],
   "source": [
    "#folder_name of the results\n",
    "setting = 'sLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_ldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "for epsilon in lst_eps:\n",
    "    print(epsilon)\n",
    "\n",
    "    ldp_acc = []\n",
    "    ldp_sp_min, ldp_sp_maj = [], []\n",
    "    ldp_tv = []\n",
    "    ldp_eo_min, ldp_eo_maj = [], []\n",
    "    ldp_eod = []\n",
    "    ldp_pe_min, ldp_pe_maj = [], []\n",
    "    ldp_ped = []\n",
    "    ldp_te_min, ldp_te_maj = [], []\n",
    "    ldp_ted = []\n",
    "    ldp_oa_min, ldp_oa_maj = [], []\n",
    "    ldp_oad = []\n",
    "    ldp_prp_min, ldp_prp_maj = [], []\n",
    "    ldp_prpd = []\n",
    "    ldp_csp_min_B0, ldp_csp_maj_B0 = [], []\n",
    "    ldp_csd_B0 = []\n",
    "    ldp_csp_min_B1, ldp_csp_maj_B1 = [], []\n",
    "    ldp_csd_B1 = []\n",
    "\n",
    "    for seed in range(nb_seed):\n",
    "\n",
    "        # Preparing X and y using original datasets\n",
    "        X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "        y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "        # Attribute's domain size\n",
    "        k = len(set(X[protected_att]))\n",
    "\n",
    "        # Cross validation using random forest\n",
    "        kf = StratifiedKFold(n_splits = n_splits, shuffle=True)\n",
    "\n",
    "        pred = []\n",
    "        pred_min = []\n",
    "        pred_maj = []\n",
    "        pred_A0_B0 = []\n",
    "        pred_A0_B1 = []\n",
    "        pred_A1_B0 = []\n",
    "        pred_A1_B1 = []\n",
    "        pred_proba_F = []\n",
    "        pred_proba_M = []\n",
    "        matrix_min = []\n",
    "        matrix_maj = []\n",
    "        test_min = []\n",
    "        test_maj = []\n",
    "        acc = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "            # Applying GRR to the protected attribute of the training set\n",
    "            X_train[protected_att] = X_train[protected_att].apply(lambda x: GRR_Client(x, k, epsilon))\n",
    "\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            # instantiate and train model\n",
    "            model = RandomForestClassifier(n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test) # prediction of the actual samples\n",
    "            pred.append(y_pred)\n",
    "            acc.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            # retrieving minority, majority from the test set\n",
    "            X_test_min, X_test_maj = X_test[X_test[protected_att] == 0], X_test[X_test[protected_att] == 1]\n",
    "\n",
    "            # predicted outcomes for minority, majority\n",
    "            y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "            pred_min.append(y_pred_min)\n",
    "            pred_maj.append(y_pred_maj)\n",
    "\n",
    "            # predicted probabilities for minority, majority\n",
    "            y_pred_proba_min, y_pred_proba_maj = model.predict_proba(X_test_min)[:,1], model.predict_proba(X_test_maj)[:,1]\n",
    "            pred_proba_F.append(y_pred_proba_min)\n",
    "            pred_proba_M.append(y_pred_proba_maj)\n",
    "\n",
    "            indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "            y_test_min, y_test_maj = y_test.get(key = indices_min), y_test.get(key = indices_maj)\n",
    "            test_min.append(y_test_min)\n",
    "            test_maj.append(y_test_maj)\n",
    "\n",
    "            # confusion matrix for minority, majority\n",
    "            conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min,y_pred_min), confusion_matrix_scorer(y_test_maj,y_pred_maj)\n",
    "            matrix_min.append(conf_matrix_min)\n",
    "            matrix_maj.append(conf_matrix_maj)\n",
    "\n",
    "            # retrieving the four groups: A0B0, A0B1, A1B0, A1B1 from the test set\n",
    "\n",
    "            X_test_min_B0, X_test_min_B1, X_test_maj_B0, X_test_maj_B1 = X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 1)],X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 1)]\n",
    "\n",
    "    # predicted outcomes for the four groups\n",
    "            y_pred_min_B0, y_pred_min_B1, y_pred_maj_B0, y_pred_maj_B1 = model.predict(X_test_min_B0), model.predict(X_test_min_B1), model.predict(X_test_maj_B0), model.predict(X_test_maj_B1)  \n",
    "            pred_A0_B0.append(y_pred_min_B0)\n",
    "            pred_A0_B1.append(y_pred_min_B1)\n",
    "            pred_A1_B0.append(y_pred_maj_B0)\n",
    "            pred_A1_B1.append(y_pred_maj_B1)\n",
    "\n",
    "        # sldp fairness metrics\n",
    "        ldp_sp_min.append(Statistical_parity(pred_min))\n",
    "        ldp_sp_maj.append(Statistical_parity(pred_maj))\n",
    "        ldp_tv.append(Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)))\n",
    "        ldp_eo_min.append(Equal_opportunity(matrix_min))\n",
    "        ldp_eo_maj.append(Equal_opportunity(matrix_maj))\n",
    "        ldp_eod.append(Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)))\n",
    "        ldp_pe_min.append(Predictive_equality(matrix_min))\n",
    "        ldp_pe_maj.append(Predictive_equality(matrix_maj))\n",
    "        ldp_ped.append(Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)))\n",
    "        ldp_te_min.append(Treatment_equality(matrix_min))\n",
    "        ldp_te_maj.append(Treatment_equality(matrix_maj))\n",
    "        ldp_ted.append(Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)))\n",
    "        ldp_oa_min.append(Overall_accuracy(test_min, pred_min))\n",
    "        ldp_oa_maj.append(Overall_accuracy(test_maj, pred_maj))\n",
    "        ldp_oad.append(Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)))\n",
    "        ldp_prp_min.append(Predictive_rate_parity(test_min, pred_min))\n",
    "        ldp_prp_maj.append(Predictive_rate_parity(test_maj, pred_maj))\n",
    "        ldp_prpd.append(Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min))) \n",
    "        ldp_acc.append(np.mean(acc))\n",
    "        ldp_csp_min_B0 = Statistical_parity(pred_A0_B0)\n",
    "        ldp_csp_maj_B0 = Statistical_parity(pred_A1_B0)\n",
    "        ldp_csd_B0.append(Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)))\n",
    "        ldp_csp_min_B1 = Statistical_parity(pred_A0_B1)\n",
    "        ldp_csp_maj_B1 = Statistical_parity(pred_A1_B1)\n",
    "        ldp_csd_B1.append(Metric_disparity(Statistical_parity(pred_A1_B1), Statistical_parity(pred_A0_B1)))\n",
    "        #writing the results to the csv file\n",
    "        write_to_csv(setting,dataset,[str(seed), str(epsilon), Statistical_parity(pred_maj),Statistical_parity(pred_min),Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)),Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min), Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)),Predictive_equality(matrix_maj), Predictive_equality(matrix_min),Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)),Treatment_equality(matrix_maj), Treatment_equality(matrix_min), Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)),Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min),Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)),Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min), Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min)),Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0),Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)), Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1),Metric_disparity(Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1)),np.mean(acc)])\n",
    "    write_to_csv(setting,dataset,[20,str(epsilon),np.std(ldp_sp_maj),np.std(ldp_sp_min),np.std(ldp_tv),np.std(ldp_eo_maj),np.std(ldp_eo_min),np.std(ldp_eod),np.std(ldp_pe_maj),np.std(ldp_pe_min),np.std(ldp_ped),np.std(ldp_te_maj),np.std(ldp_te_min),np.std(ldp_ted),np.std(ldp_oa_maj),np.std(ldp_oa_min),np.std(ldp_oad),np.std(ldp_prp_maj),np.std(ldp_prp_min),np.std(ldp_prpd),np.std(ldp_csp_maj_B0),np.std(ldp_csp_min_B0),np.std(ldp_csd_B0),np.std(ldp_csp_maj_B1),np.std(ldp_csp_min_B1),np.std(ldp_csd_B1),np.std(ldp_acc)])\n",
    "print('That took {} seconds'.format(time.time() - starttime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDP setting 2: all the sensitive attributes are obfuscated using the combined krr setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0.5\n",
      "0.1\n",
      "That took 2995.496752977371 seconds\n"
     ]
    }
   ],
   "source": [
    "#folder_name of the results\n",
    "setting = 'combsLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_ldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "lst_sensitive = dict_setting[setting]\n",
    "\n",
    "\n",
    "# New sensitive attribute (marginal of all in lst_sensitive)\n",
    "new_protected_att = '_'.join(lst_sensitive)\n",
    "\n",
    "# to compute all possible permutations\n",
    "all_list = [list(df_cp[att].unique()) for att in lst_sensitive]\n",
    "all_perm = list(itertools.product(*all_list))\n",
    "k = len(all_perm)  # new domain size\n",
    "\n",
    "df_cp[new_protected_att] = df_cp[lst_sensitive].astype(str).T.agg(', '.join)\n",
    "\n",
    "for epsilon in lst_eps:\n",
    "    print(epsilon)\n",
    "    \n",
    "    # Evaluating nb_seed times due to randomness\n",
    "    ldp_sp_min, ldp_sp_maj = [], []\n",
    "    ldp_tv = []\n",
    "    ldp_eo_min, ldp_eo_maj = [], []\n",
    "    ldp_pe_min, ldp_pe_maj = [], []\n",
    "    ldp_te_min, ldp_te_maj = [], []\n",
    "    ldp_oa_min, ldp_oa_maj = [], []\n",
    "    ldp_prp_min, ldp_prp_maj = [], []\n",
    "    ldp_acc = []\n",
    "    ldp_csp_min_B0, ldp_csp_maj_B0, ldp_csp_min_B1, ldp_csp_maj_B1 = [], [], [], []\n",
    "    ldp_csd_B1,ldp_csd_B0  = [], []\n",
    "    ldp_eod = []\n",
    "    ldp_ped = []\n",
    "    ldp_ted = []\n",
    "    ldp_oad = []\n",
    "    ldp_prpd = []\n",
    "\n",
    "\n",
    "    for seed in range(nb_seed):\n",
    "        # Preparing X and y using pandas\n",
    "        X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "        y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "        LE = LabelEncoder()\n",
    "        LE.fit([str(val).replace('(', '').replace(')', '') for val in all_perm])\n",
    "\n",
    "\n",
    "        # Cross validation (using StratifiedKFold)\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "        pred = []\n",
    "        pred_min = []\n",
    "        pred_maj = []\n",
    "        pred_proba_F = []\n",
    "        pred_proba_M = []\n",
    "        pred_A0_B0 = []\n",
    "        pred_A0_B1 = []\n",
    "        pred_A1_B0 = []\n",
    "        pred_A1_B1 = []\n",
    "        matrix_min = []\n",
    "        matrix_maj = []\n",
    "        test_min = []\n",
    "        test_maj = []\n",
    "        acc = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            if target not in lst_sensitive:\n",
    "                X_train[new_protected_att] = LE.transform(X_train[new_protected_att].astype(str))\n",
    "                # applying GRR to all sensitive attributes\n",
    "                X_train[new_protected_att] = X_train[new_protected_att].apply(lambda x: GRR_Client(x, k, epsilon))  # Applying GRR\n",
    "                X_train[new_protected_att] = LE.inverse_transform(X_train[new_protected_att])\n",
    "\n",
    "                for idx in range(len(lst_sensitive)):\n",
    "                    X_train[lst_sensitive[idx]] = X_train[new_protected_att].apply(lambda x: x.split(',')[idx]).astype(int)\n",
    "\n",
    "                del X_train[new_protected_att]  # deleting marginal\n",
    "                del X_test[new_protected_att]  # deleting marginal\n",
    "\n",
    "            else:\n",
    "                df_train = pd.concat([X_train, y_train], axis=1)\n",
    "                df_train[new_protected_att] = LE.transform(df_train[new_protected_att].astype(str))\n",
    "                df_train[new_protected_att] = df_train[new_protected_att].apply(lambda x: GRR_Client(x, k, epsilon))  # Applying GRR\n",
    "                df_train[new_protected_att] = LE.inverse_transform(df_train[new_protected_att])\n",
    "\n",
    "                for idx in range(len(lst_sensitive)):\n",
    "                    df_train[lst_sensitive[idx]] = df_train[new_protected_att].apply(lambda x: x.split(',')[idx]).astype(int)\n",
    "\n",
    "                del df_train[new_protected_att]  # deleting marginal\n",
    "                del X_test[new_protected_att]  # deleting marginal\n",
    "\n",
    "                X_train = df_train.drop(target, axis=1)\n",
    "                y_train = df_train[target]\n",
    "\n",
    "            # instantiate and train the model\n",
    "            model = RandomForestClassifier(n_jobs=-1)#, random_state=seed            \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)  # prediction of the actual samples\n",
    "            pred.append(y_pred)\n",
    "            acc.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            # retrieving minority, majority from the test set\n",
    "            X_test_min, X_test_maj = X_test[X_test[protected_att].astype(int) == 0], X_test[\n",
    "                X_test[protected_att].astype(int) == 1]\n",
    "\n",
    "            # predicted outcomes for minority, majority\n",
    "            y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "            pred_min.append(y_pred_min)\n",
    "            pred_maj.append(y_pred_maj)\n",
    "\n",
    "            # predicted probabilities for minority, majority\n",
    "            y_pred_proba_min, y_pred_proba_maj = model.predict_proba(X_test_min)[:, 1], model.predict_proba(X_test_maj)[\n",
    "                                                                                        :, 1]\n",
    "            pred_proba_F.append(y_pred_proba_min)\n",
    "            pred_proba_M.append(y_pred_proba_maj)\n",
    "\n",
    "            indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "            y_test_min, y_test_maj = y_test.get(key=indices_min), y_test.get(key=indices_maj)\n",
    "            test_min.append(y_test_min)\n",
    "            test_maj.append(y_test_maj)\n",
    "\n",
    "            # confusion matrix for minority, majority\n",
    "            conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min, y_pred_min), confusion_matrix_scorer(\n",
    "                y_test_maj, y_pred_maj)\n",
    "            matrix_min.append(conf_matrix_min)\n",
    "            matrix_maj.append(conf_matrix_maj)\n",
    "\n",
    "            # retrieving the four groups: A0B0, A0B1, A1B0, A1B1 from the test set\n",
    "            X_test_min_B0, X_test_min_B1, X_test_maj_B0, X_test_maj_B1 = X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 1)],X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 1)]\n",
    "            # predicted outcomes for the four groups\n",
    "            y_pred_min_B0, y_pred_min_B1, y_pred_maj_B0, y_pred_maj_B1 = model.predict(X_test_min_B0), model.predict(X_test_min_B1), model.predict(X_test_maj_B0), model.predict(X_test_maj_B1)  \n",
    "            pred_A0_B0.append(y_pred_min_B0)\n",
    "            pred_A0_B1.append(y_pred_min_B1)\n",
    "            pred_A1_B0.append(y_pred_maj_B0)\n",
    "            pred_A1_B1.append(y_pred_maj_B1)\n",
    "\n",
    "        # comldp fairness metrics\n",
    "        ldp_sp_min.append(Statistical_parity(pred_min))\n",
    "        ldp_sp_maj.append(Statistical_parity(pred_maj))\n",
    "        ldp_tv.append(Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)))\n",
    "        ldp_eo_min.append(Equal_opportunity(matrix_min))\n",
    "        ldp_eo_maj.append(Equal_opportunity(matrix_maj))\n",
    "        ldp_eod.append(Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)))\n",
    "        ldp_pe_min.append(Predictive_equality(matrix_min))\n",
    "        ldp_pe_maj.append(Predictive_equality(matrix_maj))\n",
    "        ldp_ped.append(Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)))\n",
    "        ldp_te_min.append(Treatment_equality(matrix_min))\n",
    "        ldp_te_maj.append(Treatment_equality(matrix_maj))\n",
    "        ldp_ted.append(Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)))\n",
    "        ldp_oa_min.append(Overall_accuracy(test_min, pred_min))\n",
    "        ldp_oa_maj.append(Overall_accuracy(test_maj, pred_maj))\n",
    "        ldp_oad.append(Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)))\n",
    "        ldp_prp_min.append(Predictive_rate_parity(test_min, pred_min))\n",
    "        ldp_prp_maj.append(Predictive_rate_parity(test_maj, pred_maj))\n",
    "        ldp_prpd.append(Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min))) \n",
    "        ldp_acc.append(np.mean(acc))\n",
    "        ldp_csp_min_B0.append(Statistical_parity(pred_A0_B0))\n",
    "        ldp_csp_maj_B0.append(Statistical_parity(pred_A1_B0))\n",
    "        ldp_csd_B0.append(Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)))\n",
    "        ldp_csp_min_B1.append(Statistical_parity(pred_A0_B1))\n",
    "        ldp_csp_maj_B1.append(Statistical_parity(pred_A1_B1))\n",
    "        ldp_csd_B1.append(Metric_disparity(Statistical_parity(pred_A1_B1), Statistical_parity(pred_A0_B1)))\n",
    "        #writing the results to the csv file\n",
    "        write_to_csv(setting,dataset,[str(seed), str(epsilon), Statistical_parity(pred_maj),Statistical_parity(pred_min),Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)),Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min), Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)),Predictive_equality(matrix_maj), Predictive_equality(matrix_min),Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)),Treatment_equality(matrix_maj), Treatment_equality(matrix_min), Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)),Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min),Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)),Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min), Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min)),Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0),Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)), Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1),Metric_disparity(Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1)),np.mean(acc)])\n",
    "    write_to_csv(setting,dataset,[20,str(epsilon),np.std(ldp_sp_maj),np.std(ldp_sp_min),np.std(ldp_tv),np.std(ldp_eo_maj),np.std(ldp_eo_min),np.std(ldp_eod),np.std(ldp_pe_maj),np.std(ldp_pe_min),np.std(ldp_ped),np.std(ldp_te_maj),np.std(ldp_te_min),np.std(ldp_ted),np.std(ldp_oa_maj),np.std(ldp_oa_min),np.std(ldp_oad),np.std(ldp_prp_maj),np.std(ldp_prp_min),np.std(ldp_prpd),np.std(ldp_csp_maj_B0),np.std(ldp_csp_min_B0),np.std(ldp_csd_B0),np.std(ldp_csp_maj_B1),np.std(ldp_csp_min_B1),np.std(ldp_csd_B1),np.std(ldp_acc)])\n",
    "print('That took {} seconds'.format(time.time() - starttime)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDP setting 3: all the sensitive attributes are obfuscated using the krr independent setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "8\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0.5\n",
      "0.1\n",
      "That took 3090.3529710769653 seconds\n"
     ]
    }
   ],
   "source": [
    "#folder_name of the results\n",
    "setting = 'indsLDP'\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "# write head of csv file\n",
    "write_to_csv(setting,dataset,header_ldp)\n",
    "\n",
    "df_cp = copy.deepcopy(df)\n",
    "\n",
    "lst_sensitive = dict_setting[setting]\n",
    "\n",
    "#d = len(lst_sensitive)  # number of attributes\n",
    "#eps_spl = epsilon / d\n",
    "\n",
    "\n",
    "# domain size (total) of sensitive attributes\n",
    "total_k = sum([len(set(df_cp[att])) for att in lst_sensitive])\n",
    "\n",
    "\n",
    "protected_att = lst_sensitive[0]\n",
    "\n",
    "\n",
    "for epsilon in lst_eps:\n",
    "    print(epsilon)\n",
    "    \n",
    "    # Evaluating nb_seed times due to randomness\n",
    "    ldp_sp_min, ldp_sp_maj = [], []\n",
    "    ldp_tv = []\n",
    "    ldp_eo_min, ldp_eo_maj = [], []\n",
    "    ldp_pe_min, ldp_pe_maj = [], []\n",
    "    ldp_te_min, ldp_te_maj = [], []\n",
    "    ldp_oa_min, ldp_oa_maj = [], []\n",
    "    ldp_prp_min, ldp_prp_maj = [], []\n",
    "    ldp_acc = []\n",
    "    ldp_csp_min_B0, ldp_csp_maj_B0, ldp_csp_min_B1, ldp_csp_maj_B1 = [], [], [], []\n",
    "    ldp_csd_B1,ldp_csd_B0  = [], []\n",
    "    ldp_eod = []\n",
    "    ldp_ped = []\n",
    "    ldp_ted = []\n",
    "    ldp_oad = []\n",
    "    ldp_prpd = []\n",
    "\n",
    "\n",
    "    for seed in range(nb_seed):\n",
    "        #np.random.seed(seed)\n",
    "\n",
    "        # Preparing X and y using pandas\n",
    "        X = copy.deepcopy(df_cp.drop(target, axis=1))\n",
    "        y = copy.deepcopy(df_cp[target])\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "        \n",
    "        pred = []\n",
    "        pred_inv = []\n",
    "        pred_min = []\n",
    "        pred_maj = []\n",
    "        pred_proba_F = []\n",
    "        pred_proba_M = []\n",
    "        pred_A0_B0 = []\n",
    "        pred_A0_B1 = []\n",
    "        pred_A1_B0 = []\n",
    "        pred_A1_B1 = []\n",
    "        matrix_min = []\n",
    "        matrix_maj = []\n",
    "        test_min = []\n",
    "        test_maj = []\n",
    "        acc = []\n",
    "\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            if target not in lst_sensitive:           \n",
    "\n",
    "                # Applying GRR to all sensitive attributes except y of training set (reminder: change how the privacy budget is distributed among vars (not even distribution)\n",
    "\n",
    "                for att in lst_sensitive:\n",
    "                    k = len(set(df_cp[att]))\n",
    "                    #X_train[att] = X_train[att].apply(lambda x: GRR_Client(x, k, eps_spl))\n",
    "                    X_train[att] = X_train[att].apply(lambda x: GRR_Client(x, k, (epsilon*k)/ total_k ))\n",
    "            else:\n",
    "\n",
    "                # Applying GRR to all sensitive attributes including y of training set\n",
    "                df_train = pandas.concat([X_train, y_train], axis=1)\n",
    "                for att in lst_sensitive:\n",
    "                    k = len(set(df_cp[att]))\n",
    "                    #df_train[att] = df_train[att].apply(lambda x: GRR_Client(x, k, eps_spl))\n",
    "                    df_train[att] = df_train[att].apply(lambda x: GRR_Client(x, k, (epsilon*k)/ total_k ))\n",
    "\n",
    "                X_train = df_train.drop(target, axis=1)\n",
    "                y_train = df_train[target]        \n",
    "\n",
    "\n",
    "            model = RandomForestClassifier(n_jobs=-1)#, random_state=seed\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test) # prediction of the actual samples\n",
    "            pred.append(y_pred)\n",
    "            acc.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "            # retrieving minority, majority from the test set\n",
    "            X_test_min, X_test_maj = X_test[X_test[protected_att] == 0], X_test[X_test[protected_att] == 1]\n",
    "\n",
    "            # predicted outcomes for minority, majority\n",
    "            y_pred_min, y_pred_maj = model.predict(X_test_min), model.predict(X_test_maj)\n",
    "            pred_min.append(y_pred_min)\n",
    "            pred_maj.append(y_pred_maj)\n",
    "\n",
    "            # predicted probabilities for minority, majority\n",
    "            y_pred_proba_min, y_pred_proba_maj = model.predict_proba(X_test_min)[:,1], model.predict_proba(X_test_maj)[:,1]\n",
    "            pred_proba_F.append(y_pred_proba_min)\n",
    "            pred_proba_M.append(y_pred_proba_maj)\n",
    "\n",
    "            indices_min, indices_maj = X_test_min.index, X_test_maj.index\n",
    "            y_test_min, y_test_maj = y_test.get(key = indices_min), y_test.get(key = indices_maj)\n",
    "            test_min.append(y_test_min)\n",
    "            test_maj.append(y_test_maj)\n",
    "\n",
    "            # confusion matrix for minority, majority\n",
    "            conf_matrix_min, conf_matrix_maj = confusion_matrix_scorer(y_test_min,y_pred_min), confusion_matrix_scorer(y_test_maj,y_pred_maj)\n",
    "            matrix_min.append(conf_matrix_min)\n",
    "            matrix_maj.append(conf_matrix_maj)\n",
    "            # retrieving the four groups: A0B0, A0B1, A1B0, A1B1 from the test set\n",
    "            X_test_min_B0, X_test_min_B1, X_test_maj_B0, X_test_maj_B1 = X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 0) & (X_test['education_num'] == 1)],X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 0)], X_test[(X_test[protected_att] == 1) & (X_test['education_num'] == 1)]\n",
    "            # predicted outcomes for the four groups\n",
    "            y_pred_min_B0, y_pred_min_B1, y_pred_maj_B0, y_pred_maj_B1 = model.predict(X_test_min_B0), model.predict(X_test_min_B1), model.predict(X_test_maj_B0), model.predict(X_test_maj_B1)  \n",
    "            pred_A0_B0.append(y_pred_min_B0)\n",
    "            pred_A0_B1.append(y_pred_min_B1)\n",
    "            pred_A1_B0.append(y_pred_maj_B0)\n",
    "            pred_A1_B1.append(y_pred_maj_B1)\n",
    "\n",
    "        #********************\n",
    "        # indldp fairness metrics\n",
    "        ldp_sp_min.append(Statistical_parity(pred_min))\n",
    "        ldp_sp_maj.append(Statistical_parity(pred_maj))\n",
    "        ldp_tv.append(Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)))\n",
    "\n",
    "        ldp_eo_min.append(Equal_opportunity(matrix_min))\n",
    "        ldp_eo_maj.append(Equal_opportunity(matrix_maj))\n",
    "        ldp_eod.append(Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)))\n",
    "\n",
    "        ldp_pe_min.append(Predictive_equality(matrix_min))\n",
    "        ldp_pe_maj.append(Predictive_equality(matrix_maj))\n",
    "        ldp_ped.append(Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)))\n",
    "\n",
    "        ldp_te_min.append(Treatment_equality(matrix_min))\n",
    "        ldp_te_maj.append(Treatment_equality(matrix_maj))\n",
    "        ldp_ted.append(Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)))\n",
    "\n",
    "        ldp_oa_min.append(Overall_accuracy(test_min, pred_min))\n",
    "        ldp_oa_maj.append(Overall_accuracy(test_maj, pred_maj))\n",
    "        ldp_oad.append(Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)))\n",
    "\n",
    "        ldp_prp_min.append(Predictive_rate_parity(test_min, pred_min))\n",
    "        ldp_prp_maj.append(Predictive_rate_parity(test_maj, pred_maj))\n",
    "        ldp_prpd.append(Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min))) \n",
    "\n",
    "        ldp_acc.append(np.mean(acc))\n",
    "        ldp_csp_min_B0.append(Statistical_parity(pred_A0_B0))\n",
    "        ldp_csp_maj_B0.append(Statistical_parity(pred_A1_B0))\n",
    "        ldp_csd_B0.append(Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)))\n",
    "\n",
    "        ldp_csp_min_B1.append(Statistical_parity(pred_A0_B1))\n",
    "        ldp_csp_maj_B1.append(Statistical_parity(pred_A1_B1))\n",
    "        ldp_csd_B1.append(Metric_disparity(Statistical_parity(pred_A1_B1), Statistical_parity(pred_A0_B1)))\n",
    "\n",
    "        #writing the results to the csv file\n",
    "        write_to_csv(setting,dataset,[str(seed), str(epsilon), Statistical_parity(pred_maj),Statistical_parity(pred_min),Metric_disparity(Statistical_parity(pred_maj), Statistical_parity(pred_min)),Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min), Metric_disparity(Equal_opportunity(matrix_maj), Equal_opportunity(matrix_min)),Predictive_equality(matrix_maj), Predictive_equality(matrix_min),Metric_disparity(Predictive_equality(matrix_maj), Predictive_equality(matrix_min)),Treatment_equality(matrix_maj), Treatment_equality(matrix_min), Metric_disparity(Treatment_equality(matrix_maj), Treatment_equality(matrix_min)),Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min),Metric_disparity(Overall_accuracy(test_maj, pred_maj), Overall_accuracy(test_min, pred_min)),Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min), Metric_disparity(Predictive_rate_parity(test_maj, pred_maj), Predictive_rate_parity(test_min, pred_min)),Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0),Metric_disparity(Statistical_parity(pred_A1_B0), Statistical_parity(pred_A0_B0)), Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1),Metric_disparity(Statistical_parity(pred_A1_B1),Statistical_parity(pred_A0_B1)),np.mean(acc)])\n",
    "    write_to_csv(setting,dataset,[20,str(epsilon),np.std(ldp_sp_maj),np.std(ldp_sp_min),np.std(ldp_tv),np.std(ldp_eo_maj),np.std(ldp_eo_min),np.std(ldp_eod),np.std(ldp_pe_maj),np.std(ldp_pe_min),np.std(ldp_ped),np.std(ldp_te_maj),np.std(ldp_te_min),np.std(ldp_ted),np.std(ldp_oa_maj),np.std(ldp_oa_min),np.std(ldp_oad),np.std(ldp_prp_maj),np.std(ldp_prp_min),np.std(ldp_prpd),np.std(ldp_csp_maj_B0),np.std(ldp_csp_min_B0),np.std(ldp_csd_B0),np.std(ldp_csp_maj_B1),np.std(ldp_csp_min_B1),np.std(ldp_csd_B1),np.std(ldp_acc)])\n",
    "print('That took {} seconds'.format(time.time() - starttime)) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
